# Sorting II - Efficient Sorts

| Efficient Sort |    Best    |   Worst    |  Average   |   Space    | Stability |
|:---------------|:----------:|:----------:|:----------:|:----------:|:---------:|
| Merge Sort     | O(n*log n) | O(n*log n) | O(n*log n) |    O(n)    |    YES    |
| Quick Sort     | O(n*log n) |   O(n^2)   | O(n*log n) | O(n*log n) |    NO     |
| Heap Sort      | O(n*log n) | O(n*log n) | O(n*log n) |    O(1)    |    YES    |

## Merge Sort

The **Merge Sort** function repeatedly divides the array into two halves until we reach a stage where we try to perform Merge Sort on a subarray of size 1 (i.e. left == right). After that, the merge function comes into play and combines the sorted arrays into larger arrays until the whole array is merged.
```
      [6 12 5 10 1 9]
        /        \
    [6 12 5]  [10 1 9]
     /   \      |   \
   [6] [12 5] [10] [1 9]
    |   /  \    \   /  \
   [6] [5][12] [10][1][9]
    |   \  /    |    \ /
   [6] [5 12]  [10] [1 9]
     \   /       \    /
    [5 6 12]    [1 9 10]
        \          /
       [1 5 6 9 10 12]
```

```py
def mergeSort(nums):
    n = len(nums)
    if n <= 1:
        return nums
    mid = n // 2
    leftSorted = mergeSort(nums[:mid])  # 0 ~ mid-1
    rightSorted = mergeSort(nums[mid:]) # mid ~ n-1
    return merge(leftSorted, rightSorted)

def merge(left, right):
    res = []
    while left and right:
        if left[0] <= right[0]:
            res.append(left.pop(0))
        else:
            res.append(right.pop(0))
    while left: 
        res.append(left.pop(0))
    while right:
        res.append(right.pop(0))
    return res
```

**Merge Sort Time Complexity Analysis**
```py
T(1) = C               # n = 1, C is constant time
T(n) = 2 * T(n/2) + K  # n > 1, K is time to merge, which is O(n)

T(n) = 2 * T(n/2) + n
     = 2 * (2 * T(n/4) + n/2) + n
     = 4 * T(n/4) + 2 * n
     = 4 * (2 * T(n/8) + n/4) + 2 * n
     = 8 * T(n/8) + 3 * n
     = 8 * (2 * T(n/16) + n/8) + 3 * n
     = 16 * T(n/16) + 4 * n
     = ......
     = 2^k * T(n/2^k) + k * n

# n/2^k means the size of the divided subproblem and k means how many times the division runs.
# When the algorithm is done, the subproblem size equals to 1 (problem cannot divide anymore).
# So we have n/2^k = 1 and thus k = logn.
T(n) = 2^k * T(n/2^k) + k * n
     = 2^logn * T(1) + logn * n
     = C * n + n * logn

# Therefore, the merge sort time complexity is O(nlogn)
```

## Quick Sort

An array is divided into sub-arrays by selecting a pivot element from the array. The pivot element should be positioned in such a way that elements less than pivot are kept on the left side and elements greater than pivot are on the right side of the pivot. The left and right sub-arrays are also divided using the same approach. This process continues until each subarray contains a single element. At this point, elements are already sorted. Finally, elements are combined to form a sorted array.
```
      [6 12 5 10 1 9*]
        /         \
   [6 5 1*]   [9 12 10*]
    /   \        /   \
  [1] [6 5*]   [9] [10 12*]
   |    |       |    /   \
  [1] [5 6]    [9] [10] [12]
   \    /       |    \   /
  [1 5 6]      [9]  [10 12]
     \           \     /
   [1 5 6]      [9 10 12]
       \           /
      [1 5 6 9 10 12]
```

```py
def partition(nums, left, right):
    pivot = nums[left]
    i, j = left + 1, right
    while i <= j:
        while i <= j and nums[i] <= pivot: i += 1
        while i <= j and nums[j] >= pivot: j -= 1
        if i <= j:
            nums[i], nums[j] = nums[j], nums[i]
    nums[left], nums[j] = nums[j], nums[left]
    return j

def qSort(nums, left, right):
    if left >= right:
        return
    p = partition(nums, left, right)
    qSort(nums, left, p-1)
    qSort(nums, p+1, right)

def quickSort(nums):
    n = len(nums)
    qSort(nums, 0, n-1)
    return nums
```

**Quick Sort Time Complexity Analysis**

- Best case scenario: the partition function is able to divide the elements into two equally half arrays every single time. We have:
```py
T(1) = C               # n = 1, C is constant time
T(n) = 2 * T(n/2) + n  # n > 1
# So this is the same as merge sort, the time complexity is O(nlogn)
```
- Worse case scenario: the partition function always divides the elements into two unequal arrays. We need to scan `2/n` elements on average for each partition and need to run partition `n` times. In this case the time complexity of Quick Sort becomes `O(n^2)`
- Average case scenario: see [Recursion Tree](<../Chapter%204%20Binary%20Trees/4.2%20Recursion%20Tree.md>)

## Heap Sort

see [Heap](<../Chapter%204%20Binary%20Trees/4.5%20Heap.md>).