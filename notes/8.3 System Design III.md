# 8.3 System Design II

## Consistent Hashing

- **Problem**: There are two challenges in data partitioning:
    - _How do we know on which node a particular piece of data will be stored?_
    - _When we add or remove nodes, how do we know what data will be moved from existing nodes to the new nodes?_ Additionally, _how can we minimize data movement when nodes join or leave?_
- **Solution**: Distributed systems can use **Consistent Hashing** to distribute data across nodes. Consistent Hashing maps data to physical nodes and ensures that only a small set of keys move when servers are added or removed.
- **Dynamo** and **Cassandra** use Consistent Hashing to distribute their data across nodes.

## Bloom Filter

- **Problem**: If we have a large set of structured data (identified by record IDs) stored in a set of data files, _what is the most efficient way to know which file might contain our required data_?
- **Solution**: The **Bloom filter** data structure tells whether an element _may be in a set, or definitely is not_. The only possible errors are false positives, i.e., a search for a nonexistent element might give an incorrect answer.
- Used in **BigTable** and **Cassandra** to reduce disk access and increase read performance.

## Checksum

- **Problem**: In a distributed system, while moving data between components, it is possible that the data fetched from a node may arrive corrupted. This corruption can occur because of faults in a storage device, network, software, etc. __How can a distributed system ensure data integrity__?
- **Solution**: Calculate a checksum and store it with data. To calculate a checksum, a cryptographic hash function like MD5, SHA-1, SHA-256, or SHA-512 is used. The hash function takes the input data and produces a string (containing letters and numbers) of fixed length; this string is called the checksum.
- **HDFS** and **Chubby** store the checksum of each file with the data.

## Quorum

- **Problem**: In Distributed Systems, data is replicated across multiple servers for fault tolerance and high availability. Once a system decides to maintain multiple copies of data, _how to make sure that all replicas are consistent_?
- **Solution**: A quorum is the minimum number of servers on which a distributed operation needs to be performed successfully before declaring the operation’s overall success. i.e., all reads and writes are not considered successful until a majority of nodes participate in the operation.
- For leader election, **Chubby** uses **Paxos**, which use quorum to ensure strong consistency.
- **Cassandra** use Quorum to ensure data consistency, each write request can be successful only if the data has been written to at least a quorum (or majority) of replica nodes.
- **Dynamo** replicates writes to a sloppy quorum of other nodes in the system, instead of a strict majority quorum like Paxos.

## Leader and Follower

- **Problem**: Quorum can lead to another problem: lower availability; at any time, the system needs to ensure that at least a majority of replicas are up and available, otherwise the operation will fail. Quorum is also not sufficient, as in certain failure scenarios, the client can still see inconsistent data.
- **Solution**: At any time, one server is elected as the leader. This **leader** becomes responsible for data replication and can act as the central point for all coordination. The **followers** only accept writes from the leader and serve as a backup. In case the leader fails, one of the followers can become the leader. In some cases, the follower can serve read requests for load balancing.
- In **Kafka**, each partition has a designated leader which is responsible for all reads and writes for that partition. A follower replicate the leader's data and can take over the leadership if the leader goes down.
- Within the **Kafka** cluster, one broker is elected as the Controller, which is responsible for admin operations, such as creating/deleting a topic, adding partitions, assigning leaders to partitions, monitoring broker failures, etc. Furthermore, the Controller periodically checks the health of other brokers in the system.
- To ensure strong consistency, **Chubby** (Paxos) performs leader election at startup. This leader is responsible for data replication and coordination.

## Write Ahead Log

- **Problem**: When the machine restarts, the program might need to know the last thing it was doing. Based on its atomicity and durability needs, the program might need to decide to redo or undo or finish what it had started. _How can the program know what it was doing before the system crash?_
- **Solution**: To guarantee durability and data integrity, each modification to the system is first written to an append-only log on the disk. This log is known as **Write-Ahead Log (WAL)** or transaction log or commit log. Writing to the WAL guarantees that if the machine crashes, the system will be able to recover and reapply the operation if necessary.
- **Cassandra**, **Kafka**, **Chubby** use WAL to write to a log file on the disk before any modifications can be applied to the system

## Segmented Log

- **Problem**: A single log can become difficult to manage. As the file grows, it can also become a performance bottleneck, especially when it is read at the startup. Older logs need to be cleaned up periodically or, in some cases, merged. _How do we handle this_?
- **Solution**: A single log file is split into multiple parts, such that the log data is divided into equal-sized log **segments**. The system can roll the log based on a rolling policy - either a configurable period of time (e.g., every 4 hours) or a configurable maximum size (e.g., every 1GB).
- **Cassandra** uses the segmented log strategy to split its commit log into multiple smaller files. A commit log segment can be archived, deleted, or recycled once all its data has been flushed to SSTables.
- **Kafka** uses log segmentation to implement storage for its partitions. A single long file could be a performance bottleneck and error-prone.

## High-Water Mark

- **Problem**: When a new leader is elected, there might be some transactions that have not been completely propagated before the old leader crashed. In such error scenarios, some followers can be missing entries in their logs, and some can have more entries than others. How do the leader and followers to know what part of the log is safe to be exposed to the clients?
- **Solution**: Keep track of the last log entry on the leader, which has been successfully replicated to a quorum of followers. The index of this entry in the log is known as the **High-Water Mark index**. The leader exposes data only up to the high-water mark index.
- **Kafka**: To deal with non-repeatable reads and ensure data consistency, Kafka brokers keep track of the high-water mark, which is the largest offset that all In-Sync-Replicas (ISRs) of a particular partition share. Consumers can see messages only until the high-water mark.

## Lease

- **Problem**: Clients often need specified lock to certain resources. If the client fails to release the lock, the resource will be locked indefinitely. This leads to resource unavailability until the system is reset. _How do we solve this problem?_
- **Solution**. Use time-bound **leases** to grant clients rights on resources. A lease is like a lock, but it works even when the client goes away. The client asks for a lease for a limited period of time, after which the lease expires. If the client wants to extend the lease, it can renew the lease before it expires.
- **Chubby** clients maintain a time-bound session lease with the leader. During this time interval, the leader guarantees to not terminate the session unilaterally.

## Heartbeat

- **Problem**: In a distributed environment, work/data is distributed among servers. To efficiently route requests in such a setup, servers need to know what other servers are part of the system, and if other servers are alive or dead.
- **Solution**: Each server periodically sends a heartbeat message to a central monitoring server or other servers in the system to show that it is still alive and functioning.
- **GFS**: The leader periodically communicates with each ChunkServer in HeartBeat messages to give instructions and collect state.
- **HDFS**: The NameNode keeps track of DataNodes through a heartbeat mechanism. Each DataNode sends periodic heartbeat messages (every few seconds) to the NameNode. If a DataNode dies, then the heartbeats to the NameNode are stopped.

## Gossip Protocol

- **Problem**: When master server is not present, maintaining a heartbeat with every other node is very expensive. _is there any other option for monitoring the state of the cluster?_
- **Solution**: **Gossip protocol** is a peer-to-peer communication mechanism in which nodes periodically exchange state information about themselves and about other nodes they know about. Each node keeps track of state information about other nodes in the cluster and share this information to one other random node every second. This way, _eventually_, each node gets to know about the state of every other node in the cluster.
- **Dynamo** and **Cassandra** use gossip protocol which allows each node to keep track of state information about the other nodes in the cluster, like which nodes are reachable, what key ranges they are responsible for, etc.

## Phi Accrual Failure Detection

- **Problem**: In distributed systems, accurately detecting failures is a hard problem to solve, as we cannot say with 100% surety if a system is genuinely down or is just very slow in responding due to heavy load, network congestion, etc. _Do we have a middle ground instead of just a output boolean?_
- **Solution**: Use adaptive failure detection algorithm as described by Phi Accrual Failure Detector. Accrual means accumulation or the act of accumulating over time. This algorithm uses historical heartbeat information to make the threshold adaptive. Instead of telling if the server is alive or not, a generic Accrual Failure Detector outputs the suspicion level about a server. A higher suspicion level means there are higher chances that the server is down.
- With Phi Accrual Failure Detector, if a node does not respond, its suspicion level is increased and could be declared dead later. As a node’s suspicion level increases, the system can gradually stop sending new requests to it. Phi Accrual Failure Detector makes a distributed system efficient as it takes into account fluctuations in the network environment and other intermittent server issues before declaring a system completely dead.
- **Cassandra** uses the Phi Accrual Failure Detector algorithm to determine the state of the nodes in the cluster.

## Split Brian

- **Problem**: When a leader of a cluster has experienced an intermittent failure and come back online becomes a **zombie leader**. The cluster has elected a new leader and the system now has two active leaders that could be issuing conflicting commands. The common scenario in which a distributed system has two or more active leaders is called **split-brain**.
- **Solution**: Every time a new leader is elected, the generation number gets incremented. This means if the old leader had a generation number of ‘1’, the new one will have ‘2’. This generation number is included in every request that is sent from the leader to other nodes. This way, nodes can now easily differentiate the real leader by simply trusting the leader with the highest number. The generation number should be persisted on disk, so that it remains available after a server reboot. One way is to store it with every entry in the Write-ahead Log.
- **Kafka** uses _Epoch number_,’ which is simply a monotonically increasing number to indicate a server’s generation.
- **ZooKeeper** is used to in **HDFS** ensure that only one NameNode is active at any time. An epoch number is maintained as part of every transaction ID to reflect the NameNode generation.
- **Cassandra** Each node stores a generation number which is incremented every time a node restarts. This generation number is included in gossip messages exchanged between nodes and is used to distinguish the current state of a node from the state before a restart.

## Fencing
- **Problem**: A slow network or a network partition can trigger a new leader election, even though the previous leader is still running and thinks it is still the active leader. Now, in this situation, if the system elects a new leader, how do we make sure that the old leader is not running and possibly issuing conflicting commands?
- **Solution**: Put a **fence** around a previously active leader so that it cannot access cluster resources and hence stop serving any read/write request.
    - Resource Fencing
    - Node Fencing
- **HDFS** uses fencing to stop the previously active NameNode from accessing cluster resources, thereby stopping it from servicing requests.

## Vector Clocks
- **Problem**: In a distributed system, different clocks tend to run at different rates, so we cannot assume that time `t` on node `A` happened before time `t+1` on node `B`.
- **Solution**: Use Vector clocks to keep track of value history and reconcile divergent histories at read time. A vector clock is effectively a `(node, counter)` pair. One vector clock is associated with every version of every object. Resolving conflicts is similar to how **Git** works. If Git can merge different versions into one, merging is done automatically. If not, the client (i.e., the developer) has to reconcile conflicts manually.
- **Dynamo** use **Vector Clocks** To reconcile concurrent updates on an object.

## Hinted Handoff
- **Problem**: a distributed system can still serve write requests (to the remaining replicas) even when a node is down. When the node which was down comes online again, how should we write data to it?
- **Solution**: For nodes that are down, the system keeps notes (or hints) of all the write requests they have missed. Once the failing nodes recover, the write requests are forwarded to them based on the hints stored.
- **Cassandra** nodes use Hinted Handoff to remember the write operation for failing nodes.
- **Dynamo** ensures that the system is _always-writeable_ by using Hinted Handoff (and Sloppy Quorum).

## Read Repair
- **Problem**: In Distributed Systems, where data is replicated across multiple nodes, some nodes can end up having stale data. How do we ensure that the node gets the latest version of the data when it is healthy again?
- **Solution**: Repair stale data during the read operation (by comparing checksum with other nodes). Once the node with old data is known, the read repair operation pushes the newer version of data to nodes with the older version.
- **Cassandra** and **Dynamo** use **Read Repair** to push the latest version of the data to nodes with the older versions.

## Merkle Trees
- **Problem**: Read Repair removes conflicts while serving read requests. But, if a replica falls significantly behind others, it might take a very long time to resolve conflicts. How can we quickly compare two copies of a range of data residing on two different replicas and figure out exactly which parts are different?
- **Solution**: A Merkle tree is a binary tree of hashes, where each internal node is the hash of its two children, and each leaf node is a hash of a portion of the original data. Comparing Merkle trees is just compare the root hashes and recursively on the left and right children.
- Dynamo uses **Merkle trees** for anti-entropy and to resolve conflicts in the background.

Reference: https://www.educative.io/courses/grokking-adv-system-design-intvw