# 8.3 System Design II

## Write Ahead Log

- **Problem**: When the machine restarts, the program might need to know the last thing it was doing. Based on its atomicity and durability needs, the program might need to decide to redo or undo or finish what it had started. _How can the program know what it was doing before the system crash?_
- **Solution**: To guarantee durability and data integrity, each modification to the system is first written to an append-only log on the disk. This log is known as **Write-Ahead Log (WAL)** or transaction log or commit log. Writing to the WAL guarantees that if the machine crashes, the system will be able to recover and reapply the operation if necessary.
- **Cassandra**, **Kafka**, **Chubby** use WAL to write to a log file on the disk before any modifications can be applied to the system

## Segmented Log

- **Problem**: A single log can become difficult to manage. As the file grows, it can also become a performance bottleneck, especially when it is read at the startup. Older logs need to be cleaned up periodically or, in some cases, merged. _How do we handle this_?
- **Solution**: A single log file is split into multiple parts, such that the log data is divided into equal-sized log **segments**. The system can roll the log based on a rolling policy - either a configurable period of time (e.g., every 4 hours) or a configurable maximum size (e.g., every 1GB).
- **Cassandra** uses the segmented log strategy to split its commit log into multiple smaller files. A commit log segment can be archived, deleted, or recycled once all its data has been flushed to SSTables.
- **Kafka** uses log segmentation to implement storage for its partitions. A single long file could be a performance bottleneck and error-prone.

## High-Water Mark

- **Problem**: When a new leader is elected, there might be some transactions that have not been completely propagated before the old leader crashed. In such error scenarios, some followers can be missing entries in their logs, and some can have more entries than others. How do the leader and followers to know what part of the log is safe to be exposed to the clients?
- **Solution**: Keep track of the last log entry on the leader, which has been successfully replicated to a quorum of followers. The index of this entry in the log is known as the **High-Water Mark index**. The leader exposes data only up to the high-water mark index.
- **Kafka**: To deal with non-repeatable reads and ensure data consistency, Kafka brokers keep track of the high-water mark, which is the largest offset that all In-Sync-Replicas (ISRs) of a particular partition share. Consumers can see messages only until the high-water mark.

## Lease

- **Problem**: Clients often need specified lock to certain resources. If the client fails to release the lock, the resource will be locked indefinitely. This leads to resource unavailability until the system is reset. _How do we solve this problem?_
- **Solution**. Use time-bound **leases** to grant clients rights on resources. A lease is like a lock, but it works even when the client goes away. The client asks for a lease for a limited period of time, after which the lease expires. If the client wants to extend the lease, it can renew the lease before it expires.
- **Chubby** clients maintain a time-bound session lease with the leader. During this time interval, the leader guarantees to not terminate the session unilaterally.

## Gossip Protocol

- **Problem**: When master server is not present, maintaining a heartbeat with every other node is very expensive. _is there any other option for monitoring the state of the cluster?_
- **Solution**: **Gossip protocol** is a peer-to-peer communication mechanism in which nodes periodically exchange state information about themselves and about other nodes they know about. Each node keeps track of state information about other nodes in the cluster and share this information to one other random node every second. This way, _eventually_, each node gets to know about the state of every other node in the cluster.
- **Dynamo** and **Cassandra** use gossip protocol which allows each node to keep track of state information about the other nodes in the cluster, like which nodes are reachable, what key ranges they are responsible for, etc.

## Phi Accrual Failure Detection

- **Problem**: In distributed systems, accurately detecting failures is a hard problem to solve, as we cannot say with 100% surety if a system is genuinely down or is just very slow in responding due to heavy load, network congestion, etc. _Do we have a middle ground instead of just a output boolean?_
- **Solution**: Use adaptive failure detection algorithm as described by Phi Accrual Failure Detector. Accrual means accumulation or the act of accumulating over time. This algorithm uses historical heartbeat information to make the threshold adaptive. Instead of telling if the server is alive or not, a generic Accrual Failure Detector outputs the suspicion level about a server. A higher suspicion level means there are higher chances that the server is down.
- With Phi Accrual Failure Detector, if a node does not respond, its suspicion level is increased and could be declared dead later. As a node’s suspicion level increases, the system can gradually stop sending new requests to it. Phi Accrual Failure Detector makes a distributed system efficient as it takes into account fluctuations in the network environment and other intermittent server issues before declaring a system completely dead.
- **Cassandra** uses the Phi Accrual Failure Detector algorithm to determine the state of the nodes in the cluster.

## Split Brian

- **Problem**: When a leader of a cluster has experienced an intermittent failure and come back online becomes a **zombie leader**. The cluster has elected a new leader and the system now has two active leaders that could be issuing conflicting commands. The common scenario in which a distributed system has two or more active leaders is called **split-brain**.
- **Solution**: Every time a new leader is elected, the generation number gets incremented. This means if the old leader had a generation number of ‘1’, the new one will have ‘2’. This generation number is included in every request that is sent from the leader to other nodes. This way, nodes can now easily differentiate the real leader by simply trusting the leader with the highest number. The generation number should be persisted on disk, so that it remains available after a server reboot. One way is to store it with every entry in the Write-ahead Log.
- **Kafka** uses _Epoch number_,’ which is simply a monotonically increasing number to indicate a server’s generation.
- **ZooKeeper** is used to in **HDFS** ensure that only one NameNode is active at any time. An epoch number is maintained as part of every transaction ID to reflect the NameNode generation.
- **Cassandra** Each node stores a generation number which is incremented every time a node restarts. This generation number is included in gossip messages exchanged between nodes and is used to distinguish the current state of a node from the state before a restart.

## Fencing
- **Problem**: A slow network or a network partition can trigger a new leader election, even though the previous leader is still running and thinks it is still the active leader. Now, in this situation, if the system elects a new leader, how do we make sure that the old leader is not running and possibly issuing conflicting commands?
- **Solution**: Put a **fence** around a previously active leader so that it cannot access cluster resources and hence stop serving any read/write request.
    - Resource Fencing
    - Node Fencing
- **HDFS** uses fencing to stop the previously active NameNode from accessing cluster resources, thereby stopping it from servicing requests.

## Vector Clocks
- **Problem**: In a distributed system, different clocks tend to run at different rates, so we cannot assume that time `t` on node `A` happened before time `t+1` on node `B`.
- **Solution**: Use Vector clocks to keep track of value history and reconcile divergent histories at read time. A vector clock is effectively a `(node, counter)` pair. One vector clock is associated with every version of every object. Resolving conflicts is similar to how **Git** works. If Git can merge different versions into one, merging is done automatically. If not, the client (i.e., the developer) has to reconcile conflicts manually.
- **Dynamo** use **Vector Clocks** To reconcile concurrent updates on an object.

## Hinted Handoff
- **Problem**: a distributed system can still serve write requests (to the remaining replicas) even when a node is down. When the node which was down comes online again, how should we write data to it?
- **Solution**: For nodes that are down, the system keeps notes (or hints) of all the write requests they have missed. Once the failing nodes recover, the write requests are forwarded to them based on the hints stored.
- **Cassandra** nodes use Hinted Handoff to remember the write operation for failing nodes.
- **Dynamo** ensures that the system is _always-writeable_ by using Hinted Handoff (and Sloppy Quorum).

## Read Repair
- **Problem**: In Distributed Systems, where data is replicated across multiple nodes, some nodes can end up having stale data. How do we ensure that the node gets the latest version of the data when it is healthy again?
- **Solution**: Repair stale data during the read operation (by comparing checksum with other nodes). Once the node with old data is known, the read repair operation pushes the newer version of data to nodes with the older version.
- **Cassandra** and **Dynamo** use **Read Repair** to push the latest version of the data to nodes with the older versions.

## Merkle Trees
- **Problem**: Read Repair removes conflicts while serving read requests. But, if a replica falls significantly behind others, it might take a very long time to resolve conflicts. How can we quickly compare two copies of a range of data residing on two different replicas and figure out exactly which parts are different?
- **Solution**: A Merkle tree is a binary tree of hashes, where each internal node is the hash of its two children, and each leaf node is a hash of a portion of the original data. Comparing Merkle trees is just compare the root hashes and recursively on the left and right children.
- Dynamo uses **Merkle trees** for anti-entropy and to resolve conflicts in the background.

Reference: https://www.educative.io/courses/grokking-adv-system-design-intvw