# 8.2 System Design II

## Key Characteristics of Distributed Systems

- Scalability (Horizontal Scaling vs. Vertical Scaling)
- Reliability (Consistency)
- Availability
- Efficiency
- Serviceability or Manageability

## Load Balancing

- LB Locations
    - Between the user and the web server
    - Between web servers and an internal platform layer, like application servers or cache servers
    - Between internal platform layer and database.
- LB Algorithms
    - Least Connection Method
    - Least Response Time Method
    - Least Bandwidth Method
    - Round Robin Method
    - Weighted Round Robin Method
    - IP Hash

## Caching

- Application server cache
- Content Delivery Network (CDN)
- Cache Invalidation
    - Write-through cache
    - Write-around cache
    - Write-back cache
- Cache eviction policies
    - First In First Out (FIFO)
    - Last In First Out (LIFO)
    - Least Recently Used (LRU)
    - Most Recently Used (MRU)
    - Least Frequently Used (LFU)
    - Random Replacement (RR)

## Data Partitioning

- Partitioning Methods
    - Horizontal Partitioning
    - Vertical Partitioning
    - Directory-Based Partitioning
- Partitioning Criteria
    - Key or Hash-based Partitioning
    - List partitioning
    - Round-robin partitioning
    - Composite Partitioning
- Common Problems of Data Partitioning
    - Joins and Denormalization
    - Referential integrity
    - Rebalancing

## Indexes

- **Indexes** provide faster random lookups through the database tables and efficient access of ordered records.
- **Indexes** can be created using one or more columns of a database table.
- **Indexes** can decrease write performance (we not only have to write the data but also have to update the index).

## Proxies

- A **forward proxy** can hide the identity of the client from the server by sending requests on behalf of the client.
- A **reverse proxy** retrieves resources from one or more servers on behalf of a client, thus hides the server’s identity.

## Fault Tolerance

- **Redundancy**: duplication of critical components or functions of a system with the intention of increasing the reliability of the system, usually in the form of a backup or fail-safe, or to improve actual system performance. It removes the single points of failure in the system
- **Replication**: share information to ensure consistency between redundant resources, such as software or hardware components, to improve reliability, fault-tolerance, or accessibility.

## SQL vs. NoSQL

- **SQL** (Relational DB)
    - MySQL, Oracle, MS SQL Server, SQLite, Postgres, and MariaDB
- **NoSQL** (Not Only SQL)
    - Key-Value Stores (Redis, Dynamo DB)
    - Document Databases (CouchDB and MongoDB)
    - Wide-Column Databases (Cassandra, HBase)
    - Graph Databases
- Reasons to use SQL
    - Ensure ACID compliance
    - Data is structured and unchanging
- Reasons to use NoSQL
    - Storing large volumes of data that often have little to no structure.
    - Making the most of cloud computing and storage (Scalability!)
    - Rapid development.

## CAP Theorem

- **CAP**
    - **Consistency** (C): All users see the same data at the same time
    - **Availability** (A): System continues to function even with node failures
    - **Partition tolerance** (P): System continues to function even if the communication fails between nodes
- Theorem
    - According to the CAP theorem, any distributed system needs to pick two out of the three properties.
    - In the presence of a network partition, therefore a distributed system must choose either Consistency or Availability.
    - CA: **RDBMS** (not a coherent option)
    - CP: **BigTable**, **HBase**
    - AP: **Dynamo**, **Cassandra**, **CouchDB**

## PACELC Theorem

- **ACID** (Atomicity, Consistency, Isolation, Durability) databases (RDBMS) chose consistency (refuse response if it cannot check with peers)
- **BASE** (Basically Available, Soft-state, Eventually consistent) databases (NoSQL) chose availability (respond with local data without ensuring it is the latest with its peers).
- **PACELC**
    - if there is a partition (P), a distributed system can tradeoff between availability (A) and consistency (C) (same as CAP)
    - else (E), when the system is running normally in the absence of partitions, the system can tradeoff between latency (L) and consistency (C).

## Client Server Interaction

- **Ajax Polling**: the client repeatedly polls (or requests) a server for data. The client makes a request and waits for the server to respond with data. If no data is available, an empty response is returned.
- **HTTP Long Polling**: allows the server to push information to a client whenever the data is available. With Long-Polling, the client requests information from the server exactly as in normal polling, but with the expectation that the server may not respond immediately.
- **WebSockets**: WebSocket provides Full duplex communication channels over a single TCP connection. It provides a persistent connection between a client and a server that both parties can use to start sending data at any time.
- **Server-Sent Events (SSEs)**: Under SSEs the client establishes a persistent and long-term connection with the server. The server uses this connection to send data to a client. If the client wants to send data to the server, it would require the use of another technology/protocol to do so.

## Consistent Hashing

- **Problem**: There are two challenges in data partitioning:
    - _How do we know on which node a particular piece of data will be stored?_
    - _When we add or remove nodes, how do we know what data will be moved from existing nodes to the new nodes?_ Additionally, _how can we minimize data movement when nodes join or leave?_
- **Solution**: Distributed systems can use **Consistent Hashing** to distribute data across nodes. Consistent Hashing maps data to physical nodes and ensures that only a small set of keys move when servers are added or removed.
- **Dynamo** and **Cassandra** use Consistent Hashing to distribute their data across nodes.

## Bloom Filter

- **Problem**: If we have a large set of structured data (identified by record IDs) stored in a set of data files, _what is the most efficient way to know which file might contain our required data_?
- **Solution**: The **Bloom filter** data structure tells whether an element _may be in a set, or definitely is not_. The only possible errors are false positives, i.e., a search for a nonexistent element might give an incorrect answer.
- Used in **BigTable** and **Cassandra** to reduce disk access and increase read performance.

## Quorum

- **Problem**: In Distributed Systems, data is replicated across multiple servers for fault tolerance and high availability. Once a system decides to maintain multiple copies of data, _how to make sure that all replicas are consistent_?
- **Solution**: A quorum is the minimum number of servers on which a distributed operation needs to be performed successfully before declaring the operation’s overall success. i.e., all reads and writes are not considered successful until a majority of nodes participate in the operation.
- For leader election, **Chubby** uses **Paxos**, which use quorum to ensure strong consistency.
- **Cassandra** use Quorum to ensure data consistency, each write request can be successful only if the data has been written to at least a quorum (or majority) of replica nodes.
- **Dynamo** replicates writes to a sloppy quorum of other nodes in the system, instead of a strict majority quorum like Paxos.

## Leader and Follower

- **Problem**: Quorum can lead to another problem: lower availability; at any time, the system needs to ensure that at least a majority of replicas are up and available, otherwise the operation will fail. Quorum is also not sufficient, as in certain failure scenarios, the client can still see inconsistent data.
- **Solution**: At any time, one server is elected as the leader. This **leader** becomes responsible for data replication and can act as the central point for all coordination. The **followers** only accept writes from the leader and serve as a backup. In case the leader fails, one of the followers can become the leader. In some cases, the follower can serve read requests for load balancing.
- In **Kafka**, each partition has a designated leader which is responsible for all reads and writes for that partition. A follower replicate the leader's data and can take over the leadership if the leader goes down.
- Within the **Kafka** cluster, one broker is elected as the Controller, which is responsible for admin operations, such as creating/deleting a topic, adding partitions, assigning leaders to partitions, monitoring broker failures, etc. Furthermore, the Controller periodically checks the health of other brokers in the system.
- To ensure strong consistency, **Chubby** (Paxos) performs leader election at startup. This leader is responsible for data replication and coordination.

## Heartbeat

- **Problem**: In a distributed environment, work/data is distributed among servers. To efficiently route requests in such a setup, servers need to know what other servers are part of the system, and if other servers are alive or dead.
- **Solution**: Each server periodically sends a heartbeat message to a central monitoring server or other servers in the system to show that it is still alive and functioning.
- **GFS**: The leader periodically communicates with each ChunkServer in HeartBeat messages to give instructions and collect state.
- **HDFS**: The NameNode keeps track of DataNodes through a heartbeat mechanism. Each DataNode sends periodic heartbeat messages (every few seconds) to the NameNode. If a DataNode dies, then the heartbeats to the NameNode are stopped.

## Checksum

- **Problem**: In a distributed system, while moving data between components, it is possible that the data fetched from a node may arrive corrupted. This corruption can occur because of faults in a storage device, network, software, etc. __How can a distributed system ensure data integrity__?
- **Solution**: Calculate a checksum and store it with data. To calculate a checksum, a cryptographic hash function like MD5, SHA-1, SHA-256, or SHA-512 is used. The hash function takes the input data and produces a string (containing letters and numbers) of fixed length; this string is called the checksum.
- **HDFS** and **Chubby** store the checksum of each file with the data.

Reference: https://www.educative.io/courses/grokking-the-system-design-interview